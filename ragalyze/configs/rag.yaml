rag:
  embedder:
    provider: dashscope
    model: text-embedding-v4
    api_key: ""
    base_url: ""
    model_kwargs:
      dimensions: 256
      encoding_format: float
    batch_size: 10
    sketch_filling: true
    force_embedding: false

  retriever:
    query-driven: true
    top_k: 20
    bm25:
      # We first use bm25 to retrieve top_k candidate documents
      # based on syntax similarity
      k1: 1.2
      b: 0.75
      # The weight of bm25 score in the final score in `normal_add`
      # 0.5 means equal contribution from both BM25 and FAISS
      weight: 0.5
    # fusion strategy includes `normal_add` and `rrf`
    # `normal_add` performs normalization before addition of faiss score and bm25 score
    # while `rrf` (Ranked Relevance Fusion) combines scores based on their rank
    fusion: "normal_add"
    rrf:
      # A smoothing constant
      k: 60
  
  text_splitter:
    split_by: token
    chunk_size: 256
    chunk_overlap: 32

  # Dynamic splitter configuration - automatically selects appropriate splitter
  dynamic_splitter:
    enabled: false
    parallel: true
    batch_size: null
    natural_language_splitter:
      split_by: word
      chunk_size: 256
      chunk_overlap: 32
    code_splitter:
      split_by: word
      chunk_size: 256
      chunk_overlap: 32

  code_understanding:
    provider: dashscope
    model: qwen3-8b
    api_key: ""
    base_url: ""
    batch_size: 10
    model_kwargs:
      temperature: 0.7
      top_p: 0.8
      max_tokens: 2048

  # Query-driven configuration
  query_driven:
    enabled: true
    top_k: 100

  # After retrieving a document, the number of adjacent documents to consider
  # For instance, if the count is 1, then consider the retrieved doc, the doc before the retrieved doc, and the doc after the retrieved doc
  adjacent_documents:
    enabled: true
    count: 1